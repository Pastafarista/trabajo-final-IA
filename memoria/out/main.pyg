    def train(pokemon1:str, pokemon2:str, epochs:int) -> None:
    agent = Agent()
    env = Environment(pokemon1, pokemon2)

    # variables para el record
    p1_wins = []
    p2_wins = []
    last_victories_p1 = 0
    last_victories_p2 = 0

    record_win_p1 = 0
    record_win_p2 = 0

    while True:
        # obtener estado antiguo
        state_old = agent.get_state(env)

        # obtener movimiento
        final_moves = agent.get_action(state_old)

        # traducir el movimiento [0, 0, 0, 1] -> 3
        rewards, done, winner  = env.step(np.argmax(final_moves[0]), np.argmax(final_moves[1]))
        state_new = agent.get_state(env)

        print(f"Partida: {agent.numero_partidas} - Recompensas P1: {rewards[0]} - Recompensa P2: {rewards[1]} - Ganador: {winner}")

        # entrenar al agente con el nuevo estado (short memory)
        agent.train_short_memory(state=state_old, actions=final_moves, rewards=rewards, next_state=state_new, done=done)

        # guardar en la memoria del agente el estado, la accion, la recompensa, el siguiente estado y si el juego ha terminado
        agent.remember(state_old, final_moves, rewards, state_new, done)

        if done:
            # entrenar al agente con todos los estados (long memory), resetear el juego y actualizar el record
            env = Environment(pokemon1, pokemon2)
            agent.numero_partidas += 1
            agent.train_long_memory()

            if winner == 0:
                last_victories_p1 += 1
            elif winner == 1:
                last_victories_p2 += 1

            if(agent.numero_partidas % 100 == 0):
                p1_wins.append(last_victories_p1 / 100)
                p2_wins.append(last_victories_p2 / 100)

                if record_win_p1 < p1_wins[-1]:
                    record_win_p1 = p1_wins[-1]
                    agent.model_p1.save(file_name=f"[{pokemon1}]-vs-{pokemon2}.pth")

                if record_win_p2 < p2_wins[-1]:
                    record_win_p2 = p2_wins[-1]
                    agent.model_p2.save(file_name=f"{pokemon2}-vs-[{pokemon1}].pth")

                last_victories_p1 = 0
                last_victories_p2 = 0

        if agent.numero_partidas == epochs:
            break

    print(f"Tiempo de entrenamiento: {end_time - start_time}")
    print(f"Mejor modelo de {pokemon1}: {record_win_p1}")
    print(f"Mejor modelo de {pokemon2}: {record_win_p2}")
\end{lstlisting}

En la primera implementaciÃ³n del entrenamiento, el modelo entrenaba contra un bot aleatorio, pero en las versiones actuales del proyecto el agente tiene dos modelos de tal forma que ambos van aprendiendo a luchar entre ellos.
